# -*- coding: utf-8 -*-
"""Hidden Markov Models for Vietnamese POS Tagging

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FK9rQlgYMvDoAE6t9kLgRhy71Bq2frQv

# Hidden Markov Models for Vietnamese POS Tagging

In this programming assignment, you will implement a first-order Hidden Markov Models for Vietnamese POS Tagging. We train the first-order HMM model on a training data and evaluate the accuracy of the model on a separate test data.

The due for the programming assignment is **March 12, 2020 (Hard deadline)**.

## Exercise 1: Write your name and student ID

Write your name and student id

- Name: Trương Sĩ Thi Vũ
- Student ID: USTHBI8-189

## Dataset

In the assignment, we will use the data obtained from Vietnamese Treebank corpus. The lecturer prepared the training/test data from the corpus.

We use 6000 labeled sentences for training and 1000 labeled for testing.

We will download two files [train.txt](https://www.dl.dropboxusercontent.com/s/xqkqmrkekb4uymv/train.txt) and [test.txt](https://www.dl.dropboxusercontent.com/s/4w98bbt4lapcdx2/test.txt)
"""

!rm -f train.txt
!rm -f test.txt
!wget https://www.dl.dropboxusercontent.com/s/xqkqmrkekb4uymv/train.txt
!wget https://www.dl.dropboxusercontent.com/s/4w98bbt4lapcdx2/test.txt

"""Each line in a file is a tokenized sentence and each token is tagged with it part-of-speech."""

!head -3 train.txt

"""### Tagset

Let's figure out how many unique tags in the training data.
"""

tagset = set()
with open('train.txt') as f:
    for line in f:
        line = line.rstrip()
        wordtags = line.split()
        for wordtag in wordtags:
            fields = wordtag.split('/')
            tag = fields[-1]
            if tag == 'chưa':
                print(wordtag)
            tagset.add(tag)

print('# Number of tags: %d' % len(tagset))

tagset

"""Meaning of some Vietnamese part-of-speech tags is described as follows.

| Tag | Description | Example |
|-----|--------------|--------------|
| A   | Adjective   | rảnh_rỗi, đẹp |
| V   | Verb | kể |
| N   | Noun | tiền, vợ_con |

## Exercise 2: Training First-Order Hidden Markov Models for POS Tagging

Please refer to the pseudo code in the lecture slide. We implement the function `train` model that read the training file, calculate parameters of HMM model ans save parameters to a model file.

Parameters of our HMM model include transition probabilities and emission probabilities. Probabilities are calculated as follows.

Transition probabilities from POS $\rightarrow$ POS.

$$
P_T(t_i|t_{i-1})=\frac{C(t_{i-1}t_i)}{C(t_{i-1})}
$$

where $C(t_{i-1})$ is number of occurences of the tag $t_{i-1}$ in the data; and $C(t_{i-1}t_i)$ is the number of times that the tag $t_{i-1}$ followed by the tag $t_i$ in our training data.

Emission probabilities are calculated as follows.

$$
P_E(w_i|t_i)=\frac{C(t_i,w_i)}{C(t_i)}
$$

You will need to complete the function `train` by writing codes after comments with `# TODO:`.
"""

from collections import defaultdict
import operator

def split_wordtag(wordtag):
    fields = wordtag.split('/')
    tag = fields.pop()
    word = '/'.join(fields)
    return word, tag


def train(train_file: str, model_file: str):
    emit = defaultdict(int)   # dictionary to store emission count C(t_i, w_i)
    transition = defaultdict(int)   # transition count C(t_{i-1}, t_i)
    context = defaultdict(int)  # count the context
    with open(train_file, 'r') as f:
        for line in f:
            line = line.strip()
            previous = '<s>'    # Make the sentence start
            context[previous] += 1
            wordtags = line.split()
            for wordtag in wordtags:
                word, tag = split_wordtag(wordtag)
                context[tag] += 1
                transition[previous + " " + tag] += 1
                emit[tag + " " + word]+= 1
                previous = tag

            transition[previous + ' </s>'] += 1   # Sentence stop

    # Now we will save the parameters of the model to a file
    with open(model_file, 'w') as fo:

        # Save transition probabilities
        for key, value in transition.items():
            
            previous, word = key.split(' ')
            fo.write('T %s %f\n' % (key, value/context[previous]))

        for key, value in emit.items():
    
          previous, word = key.split(' ')
          fo.write('E %s %f\n' % (key, (value)/context[previous])) 
        # Save emission probabilities
        # END OF YOUR CODE



train('train.txt', 'HMM_model.txt')

"""In the `train` function, we will use dict `emit` to store count values $C(t_i, w_i)$ and `transition` to store count values $C(t_{i-1}t_i)$.  **The keys of dictionaries are built by joining two strings (two POS tags or a pair of (tag, word) with a single space**.

In the model file, we save each probability in a line. In the beginning of each line, we write "T" to indicate transition probabilities and "E" for emission probabilities.

The content of the model file is something like as follows.

```
T <s> P 0.079667
T P V 0.316036
T V Nc 0.007959
T Nc N 0.829887
T N N 0.182656
...
E V liên_kế 0.000038
E N xế 0.000034
E A rề_rà 0.000139
E A quanh 0.000139
E V túa 0.000038
```

Let's call the train function.
"""

train('train.txt', 'HMM_model.txt')

"""Let's see some first lines of the model."""

!head HMM_model.txt

!tail HMM_model.txt

"""## Exercise 3: Loading HMM model from file

In the exercise 3, you need to write code to load the model file which we trained above into three dictionaries:

- `transition` is the dictionary to store transition probabilities $P_T(t_i|t_{i-1})$.
- `emission` is the dictionary to store emission probabilities $P_E(w_i|t_i)$.
- `possible_tags` is the dictionary to store unique tags in the model. It maps from a POS tag to 1.

Hint: You need to check if the line starting with 'T' or 'E' to load transition and emission probabilities.
"""

def load_model(model_file: str):
    """Load saved HMM model
    """
    transition = defaultdict(lambda: 0)
    emission = defaultdict(lambda: 0)
    possible_tags = {}
    
    # TODO: Write your code to load the model file
    # YOUR CODE HERE
    with open (model_file,'r') as f:
      for line in f:
        possible_tags[(line.split()[1])] = "word_tag"
        if line.startswith("T"):
          transition[line.split()[1] + " " + line.split()[2]] = line.split()[3]
        else:
          emission[line.split()[1] + " " + line.split()[2]] = line.split()[3]
      
    
    # END OF YOUR CODE HERE
    return transition, emission, possible_tags

"""Let's try to call the function and see the content of results."""

transition, emission, possible_tags = load_model('HMM_model.txt')
print( list(possible_tags.keys()) )
print(transition)
print(emission)

"""## Exercise 4: Viterbi algorithm

In the exercise 4, you will implement the viterbi algorithm to determine the tag sequence for a tokenized sentence given the probabilities we have loaded from the model file.
"""

import math


def viterbi(line, transition, emission, possible_tags):
    """Infer the tag sequence for a tokenized sentence

    Args:
        line (str): a tokenized word sequence
                    e.g., "Chiều cuối thu , trời vùng_biển Nghi_Xuân ảm_đạm ."
        transition (dict): transition probabilities
        emission (dict): emission probabilities
    """
    words = line.split()
    l = len(words)
    best_score = {}
    best_edge = {}
    best_score[('0 <s>')] = 0  # Start with <s>
    best_edge[('0 <s>')] = None

    import re

    # Forward Step
    for i in range(l):
        for prev in possible_tags.keys(): 
            for _next in possible_tags.keys():               
                if str(i) + ' ' + prev in best_score and prev + ' ' + _next in transition:
                    if emission[_next + ' ' + words[i]] == 0:
                      if words[i].isnumeric(): #numbers filer
                        if _next == "M":
                          emission[_next + " " + words[i]] = 1
                        else: 
                          emission[_next + " " + words[i]] = 10 ** (-10) 
                      elif words[i][0].isupper() and re.match('^[a-zA-Z_]+$', words[i]): #foreign nouns, not 100% correct
                        if _next == "NNP":
                          emission[_next + " " + words[i]] = 1
                        else: 
                          emission[_next + " " + words[i]] = 10 ** (-10) 
                      else:
                          emission[_next + " " + words[i]] = 10 ** (-10)
                    #Score = best score of "i prev" - log(T(prev|next)) - log(E(next|word[i]))
                    score = best_score[str(i) + " " + prev] + (- math.log(float(transition[prev + " " + _next]))) + (- math.log(float(emission[_next + " " + words[i]])))

                    if str(i + 1) + " " + _next not in best_score or best_score[str(i + 1) + " " + _next] > score:    
                        best_score[str(i + 1) + " " + _next] = score       
                        best_edge[str(i + 1) + " " + _next] = str(i) + " " + prev
                 
    for prev in possible_tags.keys():
        if str(l) + ' ' + prev in best_score:
            if (prev + ' ' + '</s>') not in transition:
                transition[prev + ' ' + '</s>'] = 10 ** (-10)
            #Score = best score of "l prev" - log(T(</s> | prev))
            score = best_score[str(l) + " " + prev] - math.log(float(transition[prev + ' ' + '</s>']))
          
            if str(l+1) + " " + "</s>" not in best_score or best_score[str(l+1) + ' </s>'] > score:
        
              best_score[str(l+1) + ' </s>'] = score
              best_edge[str(l+1) + ' </s>'] = str(l) + " " + prev
       
   
    # Backward Step
    tags = []
    next_edge = best_edge[str(l+1) + " " + "</s>"]
  
    while next_edge != "0 <s>":
      position, tag = next_edge.split()
      tags.append(tag)
      next_edge = best_edge[next_edge]
    tags.reverse()
    return ' '.join(tags)

"""Let's try to use the function viterbi function to find the tag sequence for a given sentence."""

print(viterbi('Ông Thivu có 300483 nhân viên .', transition, emission, possible_tags))
print(viterbi('Chiều cuối thu , trời vùng_biển Nghi_Xuân ảm_đạm .', transition, emission, possible_tags))

"""You should get the tag sequence, let\'s say \'N N V CH N N NNP A CH\'.

## Exercise 5: Model Evaluation

In the exercise, you do not not to write code. Just understand and execute the code.

We will evaluate the model on the test data with accuracy measure.
"""

def split_wordtag(wordtag):
    fields = wordtag.split('/')
    tag = fields.pop()
    word = '/'.join(fields)
    return word, tag


def evaluate(test_file: str, transition, emission, possible_tags):
    correct = 0
    total = 0

    with open(test_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line == '':
                continue
            wordtags = line.split()
            words = []
            gold_tags = []
            for wordtag in wordtags:
                word, tag = split_wordtag(wordtag)
                words.append(word)
                gold_tags.append(tag)
            sentence = ' '.join(words)
            predicted_tags = viterbi(sentence, transition, emission, possible_tags)
            predicted_tags = predicted_tags.split(' ')
            for t1, t2 in zip(predicted_tags, gold_tags):
                total += 1
                if t1 == t2:
                    correct += 1
    return 100.0 * correct/total, correct, total

acc, correct, total = evaluate('test.txt', transition, emission, possible_tags)
print("Accuracy = {} ({}/{})".format(acc, correct, total))

"""You should get the accuracy about 86.9%.

## Bonus exercises

- You will get bonus points if you can use some advanced techniques for dealing with unknown words or adding more features into the HMM model.
- You will get bonus points if you can obtain better accuracy than 87% using HMM model. For instance, you can implement the second-order HMM tagger.

## I directly added a filter for the model above (I put some codes above) that deals with new numbers and foreign NNP. I only got the accuracy reach 87.81%. I would like to apply second-order HMM but the second order viterbi is a bit hard.

## I also tried Laplace smoothing but the accuracy decreases
"""