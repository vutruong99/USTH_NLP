# -*- coding: utf-8 -*-
"""Naive Bayes for Sentiment Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19hk5gb0rwiCNjL6dv29ShESxUJ4ovd74

# Naive Bayes for Sentiment Classification

In this notebook, we will implement Naive Bayes algorithm for text classification. We will use sentiment classification data in the notebook.

Student information

#Name: Trương Sĩ Thi Vũ

#ID: USTHBI8-189

## Data

We will use the sentiment analysis corpus in [polarity dataset v1.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt) from [Moview Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/) created by Bo Pang and Lillian Lee. The task is to classify reviews into positive or negative polarity.

Dataset contains 10662 reviews of movies in which 50% of reviews have positive sentiment and 50% of reviews have negative sentiment. Data is stored in the file `sentiment.txt` in which each line is a review with labels (+1 or -1) at the beginning. All reviews are tokenized. For instance.

```
+1 if you sometimes like to go to the movies to have fun , wasabi is a good place to start . 
-1 enigma is well-made , but it's just too dry and too placid .
```

We need to download data first.
"""

!rm -f sentiment.txt
!wget https://raw.githubusercontent.com/minhpqn/nlp_100_drill_exercises/master/data/sentiment.txt

"""### Loading data

We will load data into a list of sentences with their labels.
"""

import re


def load_data(file_path):
    data = []
    # Regular expression to get the label and the text
    regx = re.compile(r'^(\+1|-1)\s+(.+)$')
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if line == '':
                continue
            match = regx.match(line)
            if match:
                lb = match.group(1)
                text = match.group(2)
                data.append((text, lb))
    return data

data = load_data('./sentiment.txt')

print(data[0])
print(data[-1])

"""## Train/test split

We will split the data into train/test so that the label distributions on two data files are similar. We will split data with the ratio 80/20.

We use [scikit-learn](https://scikit-learn.org) library to do train/test split.
"""

from sklearn.model_selection import train_test_split

texts, labels = zip(*data)
train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)

"""Let's check labels on the training data and test data."""

from collections import Counter

print(Counter(train_labels))
print(Counter(test_labels))

"""## Multinomial Naive Bayes Model

In this section, we will implement the Multinomial Naive Bayes (MNB) model. The implementation follows the pseudo code if Figure 4.2, chapter 4 "Naive Bayes and Sentiment Analysis" (SLP Book).

### Training Multinomial Naive Bayes Model

We first extract a vocabulary from a training dataset which is a list of sentences. For the sake of simplicity, we extract all words except punctuations.
"""

import string

def build_vocab(texts):
    """Build vocabulary from dataset

    Args:
        texts (list): list of tokenized sentences
    
    Returns:
        vocab (dict): map from word to index
    """
    vocab = {}
    negation = ["n't", "not", "no", "never"]
    flag = False 
    for s in texts:
        for word in s.split():
            # Check if word is a punctuation
            if word in string.punctuation:
                flag = False
                continue
            if word not in vocab:
                if flag == True:
                    word = "NOT_" + word
                idx = len(vocab)
                vocab[word] = idx

                if negation[0] in word or any(neg == word for neg in negation):
                    flag = True
    return vocab

"""Let's check how the function `build_vocab` works."""

vocab = build_vocab(train_texts)

print(vocab)

from collections import defaultdict
import math


def train_naive_bayes(texts, labels, target_classes, alpha=1):
    """Train a multinomial Naive Bayes model
    """
    ndoc = 0
    nc = defaultdict(int)   # map from a class label to number of documents in the class
    logprior = dict()
    loglikelihood = dict()
    count = defaultdict(int)  # count the occurrences of w in documents of class c

    newData = []
    vocab = build_vocab(texts)
    print(vocab)
    # Training
    for s, c in zip(texts, labels):
        line = []
        buffer = ""
        for w in s.split():
            if w in buffer.split():
              continue
            else:
              buffer = buffer + w + " "
        line.append(buffer.strip())
        line.append(c)
        newData.append(line)

    newTexts, newLabels = zip(*newData)
    print(newTexts)
    for s, c in zip(newTexts, newLabels):

        index = 0

        ndoc += 1
        nc[c] += 1
        for w in s.split():
            if w in vocab:
                count[(w,c)] += 1
            index += 1

    print(s)
    vocab_size = len(vocab)
    for c in target_classes:
        logprior[c] = math.log(nc[c]/ndoc)
        sum_ = 0
        for w in vocab.keys():
            if (w,c) not in count: count[(w,c)] = 0
            sum_ += count[(w,c)]
        
        for w in vocab.keys():
            loglikelihood[(w,c)] = math.log( (count[(w,c)] + alpha) / (sum_ + alpha * vocab_size) )
    
    return logprior, loglikelihood, vocab

"""Let's test the train function on a toy example"""

data = [
    ("Chinese Beijing Chinese", "c"),
    ("Chinese Chinese Shanghai", "c"),
    ("Chinese Macao", "c"),
    ("Tokyo Japan Chinese", "j")
]
texts, labels = zip(*data)
target_classes = ["c", "j"]

logprior, loglikelihood, vocab = train_naive_bayes(texts, labels, target_classes)

"""Let's confirm our implementation works correctly."""

assert logprior['c'] == math.log(0.75)
assert logprior['j'] == math.log(0.25)
assert loglikelihood[('Chinese', 'c')] == math.log(4/12)
assert loglikelihood[('Tokyo', 'c')] == math.log(1/12)
assert loglikelihood[('Japan', 'c')] == math.log(1/12)
assert loglikelihood[('Tokyo', 'j')] == math.log(2/9)

"""There is no assert exception, so our implementation of the training step is correct!

#### Prediction Function
"""

def test_naive_bayes(testdoc, logprior, loglikelihood, target_classes, vocab):
    sum_ = {}
    negation = ["n't", "not", "no", "never"]
    flag = False
    for c in  target_classes:
        sum_[c] = logprior[c]
        for w in testdoc.split():
            if flag == True:
                w = "NOT_" + w
            if negation[0] in w or any(neg == w for neg in negation):
                flag = True
            if w in vocab:
                sum_[c] += loglikelihood[(w,c)]
    # sort keys in sum_ by value
    sorted_keys = sorted(sum_.keys(), key=lambda x: sum_[x], reverse=True)
    return sorted_keys[0]

"""Let's try to predict the label for a test document."""

print('Predicted class: %s' % test_naive_bayes('Chinese Chinese Tokyo Japan', logprior, loglikelihood, target_classes, vocab))

"""Now, it is time to train our Naive Bayes model on the sentiment data."""

target_classes = ['+1', '-1']    # we can construct a fixed set of classes from train_labels
logprior, loglikelihood, vocab = train_naive_bayes(train_texts, train_labels, target_classes)

test_naive_bayes("enigma is well-made , but it's just too dry and too placid .", logprior, loglikelihood, target_classes, vocab)

"""### Evaluation

We will calculate evaluation measures on the test data. You can implement evaluation measures by yourself, but in this notebook, we are going to use scikit-learn to do that.

Let's get predicted classes of test documents.
"""

predicted_labels = [test_naive_bayes(s, logprior, loglikelihood, target_classes, vocab)
                    for s in test_texts]

from sklearn import metrics

print('Accuracy score: %f' % metrics.accuracy_score(test_labels, predicted_labels))

"""We can calculate precision, recall, f1_score per class."""

for c in target_classes:
    print('Evaluation measures for class %s' % c)
    print('  Precision: %f' % metrics.precision_score(test_labels, predicted_labels, pos_label=c))
    print('  Recall: %f' % metrics.recall_score(test_labels, predicted_labels, pos_label=c))
    print('  F1: %f' % metrics.f1_score(test_labels, predicted_labels, pos_label=c))

"""We can also compute macro-averaged and micro-averaged f1 score."""

print('Macro-averaged f1: %f' % metrics.f1_score(test_labels, predicted_labels, average='macro'))
print('Micro-averaged f1: %f' % metrics.f1_score(test_labels, predicted_labels, average='micro'))

"""We can report classification results all by once."""

print(metrics.classification_report(test_labels, predicted_labels))

"""## Programming Assignment 3

**Due date: March 24, 2020**

- Modify the implementation of train/test function for Boolean Multinomial Naive Bayes model and evaluate it on the test dataset of sentiment analysis data.
- (Optional, Bonus Points) Try to deal with negation (by adding NOT_ for the words after negation words).
- (Optional, Bonus Points) Try sentiment lexicon in feature extraction

What to submit:
- Link to Google Colab, please share the reading permission for me (minhpham0902@gmail.com)
- Write your name, student id in your Google Colab.
"""

