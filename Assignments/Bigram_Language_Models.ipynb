{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bigram Language Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkvuVLmc7SN3",
        "colab_type": "text"
      },
      "source": [
        "###Full name: **Trương Sĩ Thi Vũ**\n",
        "###Student number: **USTHBI8-189**\n",
        "###Email address: **vutst.b8189@st.usth.edu.vn**\n",
        "\n",
        "Please run each cell to observe the final results. (Entropy: 11.284992503675998\n",
        "Perplexity: 2495.2912543003954)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZxKE4cMUcH3",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "\n",
        "We will use the file [wiki-en-train.word](https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word) as the training data, and [wiki-en-test.\n",
        "word](https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word) as the test data. To test our implementation quickly, we will use small data file [02-train-input.txt](https://github.com/neubig/nlptutorial/blob/master/test/02-train-input.txt). All data files are from the [nlptutorial](https://github.com/neubig/nlptutorial) by Graham Neubig.\n",
        "\n",
        "As the first step, we will download all necessary data files using `wget` command line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5JRqLFIVnI-",
        "colab_type": "code",
        "outputId": "b08220dc-ba97-4a6f-9af5-23466b01cdd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        }
      },
      "source": [
        "!rm -f wiki-en-train.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
        "    \n",
        "!rm -f wiki-en-test.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
        "\n",
        "!rm -f 02-train-input.txt\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-08 16:55:56--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203886 (199K) [text/plain]\n",
            "Saving to: ‘wiki-en-train.word’\n",
            "\n",
            "wiki-en-train.word  100%[===================>] 199.11K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-02-08 16:55:58 (4.35 MB/s) - ‘wiki-en-train.word’ saved [203886/203886]\n",
            "\n",
            "--2020-02-08 16:56:07--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26989 (26K) [text/plain]\n",
            "Saving to: ‘wiki-en-test.word’\n",
            "\n",
            "wiki-en-test.word   100%[===================>]  26.36K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-02-08 16:56:08 (2.12 MB/s) - ‘wiki-en-test.word’ saved [26989/26989]\n",
            "\n",
            "--2020-02-08 16:56:16--  https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12 [text/plain]\n",
            "Saving to: ‘02-train-input.txt’\n",
            "\n",
            "02-train-input.txt  100%[===================>]      12  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-08 16:56:16 (2.97 MB/s) - ‘02-train-input.txt’ saved [12/12]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0WLgNXeVt0Z",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Estimating probabilities\n",
        "\n",
        "What you need to do in this part is complete the function `train_bigram` to estimate unigram, bigram probabilities (using MLE method) from a text file and save probabilities to a model file.\n",
        "\n",
        "The format of the model file is as follows. Each line includes an n-gram (unigram or bigram) and its probability estimated by MLE method.\n",
        "\n",
        "```\n",
        "<s> a\t1.000000\n",
        "a\t0.250000\n",
        "a b\t1.000000\n",
        "b\t0.250000\n",
        "b c\t0.500000\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP2Tj-kZjowV",
        "colab_type": "text"
      },
      "source": [
        "### Part 1.1: Function train_bigram()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX2a0Yetfzta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def train_bigram(train_file, model_file):\n",
        "    \"\"\"Train trigram language model and save to model file\n",
        "    \"\"\"\n",
        "    counts = defaultdict(int)  # count the n-gram\n",
        "    word_count = 0 # total words (V)\n",
        "    context_counts = defaultdict(int)   # count the context\n",
        "    with open(train_file) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "\n",
        "    \n",
        "            for i in range(1, len(words)):  \n",
        "                word_count += 1\n",
        "                counts[words[i]] +=1\n",
        "                counts[words[i-1]+ \" \"+ words[i]] += 1\n",
        "                context_counts[words[i-1]] +=1\n",
        "                # print(\"The probability of getting {} if already had {} is {}\".format(words[i],words[i-1],(counts[words[i-1]+ \" \" + words[i]]/context_counts[words[i-1]])))\n",
        "                pass\n",
        "   \n",
        "\n",
        "\n",
        "    # Save probabilities to the model file            \n",
        "    with open(model_file, 'w') as fo:\n",
        "        for ngram, count in counts.items():\n",
        "           \n",
        "            if len(ngram.split()) > 1:\n",
        "              fo.write(ngram + \"\\t\" + \"{}\".format(counts[ngram]/context_counts[ngram.split()[0]])+\"\\n\")\n",
        "            else: fo.write((ngram + \"\\t\" + \"{}\".format(counts[ngram]/word_count))+\"\\n\")\n",
        "            \n",
        "            pass\n",
        "\n",
        "train_bigram(\"wiki-en-train.word\",\"02-train-answer.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpbTBe8xi1rH",
        "colab_type": "text"
      },
      "source": [
        "Let's try to train bigram model on the small data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lmBGnZWi5_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bigram('02-train-input.txt', '02-train-answer.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xofAVqrNjEHA",
        "colab_type": "text"
      },
      "source": [
        "Let's see the content of the model. After completing the function `train_bigram`, you should see. The order of lines may be different.\n",
        "\n",
        "```\n",
        "</s>\t0.250000\n",
        "<s> a\t1.000000\n",
        "a\t0.250000\n",
        "a b\t1.000000\n",
        "b\t0.250000\n",
        "b c\t0.500000\n",
        "b d\t0.500000\n",
        "c\t0.125000\n",
        "c </s>\t1.000000\n",
        "d\t0.125000\n",
        "d </s>\t1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Wf9fXyjLwO",
        "colab_type": "code",
        "outputId": "1245116f-f6ad-4885-82d6-134840099e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "!cat 02-train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a\t0.25\n",
            "<s> a\t1.0\n",
            "b\t0.25\n",
            "a b\t1.0\n",
            "c\t0.125\n",
            "b c\t0.5\n",
            "</s>\t0.25\n",
            "c </s>\t1.0\n",
            "d\t0.125\n",
            "b d\t0.5\n",
            "d </s>\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqpiLpxjM5z",
        "colab_type": "text"
      },
      "source": [
        "### Part 1.2: load the model file\n",
        "\n",
        "We are going to implement the function `load_bigram_model` to load the model file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0lc9hiMkAJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_bigram_model(model_file):\n",
        "  \n",
        "    probs = {}\n",
        "    with open(model_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            probs[line.split(\"\\t\")[0]] = float(line.split(\"\\t\")[1])\n",
        "            pass\n",
        "    return probs\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czcC1vOIq3YV",
        "colab_type": "text"
      },
      "source": [
        "Let's test the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3pTilwdq5lP",
        "colab_type": "code",
        "outputId": "685465d8-75dd-40b3-85fb-83ec27fe504f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(load_bigram_model('02-train-answer.txt'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 0.25, '<s> a': 1.0, 'b': 0.25, 'a b': 1.0, 'c': 0.125, 'b c': 0.5, '</s>': 0.25, 'c </s>': 1.0, 'd': 0.125, 'b d': 0.5, 'd </s>': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBpjGSG8ky7u",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Evaluating Bigram Language Model\n",
        "\n",
        "In this part, we will evaluate the bigram language model on the test set. We will use linear interpolation as the smoothing technique.\n",
        "\n",
        "What we need to do is to complete the function  `test_bigram` as follows. The function will return perplexity on the test data.\n",
        "\n",
        "Recall that, the smoothed bigram probabilities using interpolation technique is calculated as follows.\n",
        "\n",
        "Bigrams:\n",
        "\n",
        "$$\n",
        "P(w_i|w_{i-1})=\\lambda_2 \\times P_{ML}(w_i|w_{i-1})+(1-\\lambda_2)\\times P(w_i)\n",
        "$$\n",
        "\n",
        "where $P(w_i)$ is the smoothed unigram probability and calculated as follows.\n",
        "\n",
        "$$\n",
        "P(w_i)=\\lambda_1 \\times P_{ML}(w_i) + (1-\\lambda_1) \\times \\frac{1}{N}\n",
        "$$\n",
        "\n",
        "where $N$ is a large number, e.g., $N=1,000,000$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLb5kGJilgrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def test_bigram(test_file, model_file, lambda2=0.95, lambda1=0.95, N=1000000):\n",
        "    W = 0 # Total word count\n",
        "    H = 0\n",
        "\n",
        "    probs = load_bigram_model(model_file)\n",
        "    with open(test_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "            for i in range(1, len(words)):  \n",
        "                if (probs.get(words[i])):\n",
        "                  p1 = lambda1 * probs.get(words[i]) + (1-lambda1) * (1/N)\n",
        "                else: \n",
        "                  p1 = (1-lambda1) * (1/N) \n",
        "                if (probs.get(words[i-1] + \" \" + words[i])):\n",
        "                  p2 = lambda2 * probs.get(words[i-1] + \" \" + words[i])+ (1-lambda2) * p1\n",
        "                else:\n",
        "                  p2 = (1-lambda2) * p1\n",
        "\n",
        "                W += 1  # Count the words\n",
        "                H += -math.log2(p2) # We use logarithm to avoid underflow\n",
        "    H = H/W\n",
        "    P = 2**H\n",
        "    \n",
        "    print(\"Entropy: {}\".format(H))\n",
        "    print(\"Perplexity: {}\".format(P))\n",
        "\n",
        "    return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To39tEQ0pAH6",
        "colab_type": "text"
      },
      "source": [
        "Now let's calculate on the Wikipedia data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHAE96uvpQPR",
        "colab_type": "code",
        "outputId": "30877294-7980-4690-e61e-eaf23c60b468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_bigram('wiki-en-train.word', 'bigram_model.txt')\n",
        "test_bigram('wiki-en-test.word', 'bigram_model.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy: 11.284859117885485\n",
            "Perplexity: 2495.0605603552463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQcvtIM1wRh-",
        "colab_type": "text"
      },
      "source": [
        "You should get entropy value about 11.28."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiyicE5hpUyb",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)"
      ]
    }
  ]
}